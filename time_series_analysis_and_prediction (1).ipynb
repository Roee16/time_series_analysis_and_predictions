{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60bb86c-57cd-48b0-ae84-9ff121eb398b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "time series analysis and predict\n",
    "latest methods for time series analysis to predict stock prices, along with Python code to simulate them\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768d92ae-3d95-4826-81d7-7c1e71ee0cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('stock_prices.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Fit ARIMA model\n",
    "model = ARIMA(data['Close'], order=(5, 1, 0))\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Forecast\n",
    "forecast = model_fit.forecast(steps=30)\n",
    "print(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7680e8-8746-4e5d-bccd-dec1f85d656e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM is a type of recurrent neural network (RNN) that is effective for time series prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8433ca-2250-4f92-9027-64e7615758d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('stock_prices.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "data = data['Close'].values.reshape(-1, 1)\n",
    "\n",
    "# Scale data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Create training and test sets\n",
    "train_data = scaled_data[:int(len(scaled_data)*0.8)]\n",
    "test_data = scaled_data[int(len(scaled_data)*0.8):]\n",
    "\n",
    "# Create dataset for LSTM\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        X.append(a)\n",
    "        Y.append(dataset[i + look_back, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "look_back = 1\n",
    "X_train, y_train = create_dataset(train_data, look_back)\n",
    "X_test, y_test = create_dataset(test_data, look_back)\n",
    "\n",
    "# Reshape input to be [samples, time steps, features]\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Build LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "model.add(LSTM(50, return_sequences=False))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=2)\n",
    "\n",
    "# Predict\n",
    "predicted_stock_price = model.predict(X_test)\n",
    "predicted_stock_price = scaler.inverse_transform(predicted_stock_price)\n",
    "print(predicted_stock_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677865d6-aaa1-4848-81d5-aa65fdefaab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holt-Winters Exponential Smoothing\n",
    "# This method is used for forecasting time series data with a seasonal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7f215d-addf-4d7d-a190-748848a82d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('stock_prices.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Fit Holt-Winters model\n",
    "model = ExponentialSmoothing(data['Close'], seasonal='add', seasonal_periods=12)\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Forecast\n",
    "forecast = model_fit.forecast(steps=30)\n",
    "print(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dd2f72-363f-4340-afb7-a1bfbfbc8ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest is an ensemble learning method for regression and classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5121421a-5678-42c9-88f5-88d59c3632c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('stock_prices.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Prepare data\n",
    "X = data[['Open', 'High', 'Low', 'Volume']]\n",
    "y = data['Close']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Random Forest model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d9148a-5be8-4a61-9ec6-615845e6e102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This approach combines multiple models to improve prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db53d817-8b9c-4d9f-802b-dc43713f7c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('stock_prices.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Prepare data\n",
    "X = data[['Open', 'High', 'Low', 'Volume']]\n",
    "y = data['Close']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define base models\n",
    "estimators = [\n",
    "    ('rf', RandomForestRegressor(n_estimators=100, random_state=42)),\n",
    "    ('lr', LinearRegression())\n",
    "]\n",
    "\n",
    "# Define stacking ensemble model\n",
    "stacked_model = StackingRegressor(estimators=estimators, final_estimator=RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "stacked_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "predictions = stacked_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188b78a0-86b7-452a-a686-a3ccfd47b7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gated Recurrent Unit (GRU)\n",
    "# GRU is another type of recurrent neural network similar to LSTM but with fewer parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a246b005-4186-4604-b438-7a1ae86836b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, Dense\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('stock_prices.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "data = data['Close'].values.reshape(-1, 1)\n",
    "\n",
    "# Scale data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Create training and test sets\n",
    "train_data = scaled_data[:int(len(scaled_data)*0.8)]\n",
    "test_data = scaled_data[int(len(scaled_data)*0.8):]\n",
    "\n",
    "# Create dataset for GRU\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        X.append(a)\n",
    "        Y.append(dataset[i + look_back, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "look_back = 1\n",
    "X_train, y_train = create_dataset(train_data, look_back)\n",
    "X_test, y_test = create_dataset(test_data, look_back)\n",
    "\n",
    "# Reshape input to be [samples, time steps, features]\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Build GRU model\n",
    "model = Sequential()\n",
    "model.add(GRU(50, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "model.add(GRU(50, return_sequences=False))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=2)\n",
    "\n",
    "# Predict\n",
    "predicted_stock_price = model.predict(X_test)\n",
    "predicted_stock_price = scaler.inverse_transform(predicted_stock_price)\n",
    "print(predicted_stock_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd7a075-7fa0-4bff-ba5e-3594e8ce35e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generative Adversarial Networks (GANs)\n",
    "# GANs can be used to generate synthetic stock price data and improve prediction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d4262e-6ccf-42e0-8ad0-1b53a1496703",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "# Define the generator model\n",
    "def build_generator():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=100, activation='relu'))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    return model\n",
    "\n",
    "# Define the discriminator model\n",
    "def build_discriminator():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1024, input_dim=1, activation='relu'))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# Define the GAN model\n",
    "def build_gan(generator, discriminator):\n",
    "    model = Sequential()\n",
    "    model.add(generator)\n",
    "    model.add(discriminator)\n",
    "    return model\n",
    "\n",
    "# Compile models\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "gan = build_gan(generator, discriminator)\n",
    "gan.compile(optimizer=Adam(lr=0.0002, beta_1=0.5), loss='binary_crossentropy')\n",
    "discriminator.compile(optimizer=Adam(lr=0.0002, beta_1=0.5), loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e4bb11-d3ff-4d5b-9089-b3eca90e6a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facebook Prophet\n",
    "# Prophet is a procedure for forecasting time series data based on an additive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3af6fa7-3d64-486d-9f82-269dd78d9162",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet import Prophet\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('stock_prices.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.rename(columns={'Close': 'y', 'Date': 'ds'}, inplace=True)\n",
    "\n",
    "# Fit Prophet model\n",
    "model = Prophet()\n",
    "model.fit(data)\n",
    "\n",
    "# Make a future dataframe for predictions\n",
    "future = model.make_future_dataframe(periods=30)\n",
    "\n",
    "# Predict\n",
    "forecast = model.predict(future)\n",
    "print(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb157b98-97ad-4a10-835d-5a6968829f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost is an efficient and scalable implementation of gradient boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d8494e-ec84-4a5f-839b-09a88a93fb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('stock_prices.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Prepare data\n",
    "X = data[['Open', 'High', 'Low', 'Volume']]\n",
    "y = data['Close']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train XGBoost model\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc15e74a-0338-4350-9e6c-e4f6a6f78550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sentiment analysis on news articles or social media to predict stock price movements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c240676b-40de-4256-b60f-8d42e07a1937",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('stock_prices.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Sentiment analysis\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "data['sentiment'] = data['News'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
    "\n",
    "# Use sentiment as a feature in your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e36dba-40e8-404a-9e93-aead1276135c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid ARIMA-LSTM Model\n",
    "# Combine the strengths of ARIMA for linear trends and LSTM for capturing nonlinear patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef0eacf-d2b5-4d9a-b98b-2575036c3511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('stock_prices.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# ARIMA model\n",
    "arima_model = ARIMA(data['Close'], order=(5, 1, 0))\n",
    "arima_fit = arima_model.fit()\n",
    "\n",
    "# LSTM model\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(data['Close'].values.reshape(-1, 1))\n",
    "\n",
    "look_back = 1\n",
    "X, y = [], []\n",
    "for i in range(len(scaled_data)-look_back-1):\n",
    "    X.append(scaled_data[i:(i+look_back), 0])\n",
    "    y.append(scaled_data[i + look_back, 0])\n",
    "X, y = np.array(X), np.array(y)\n",
    "X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(50, return_sequences=True, input_shape=(X.shape[1], 1)))\n",
    "lstm_model.add(LSTM(50, return_sequences=False))\n",
    "lstm_model.add(Dense(1))\n",
    "lstm_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "lstm_model.fit(X, y, epochs=100, batch_size=32, verbose=2)\n",
    "\n",
    "# Forecast\n",
    "arima_forecast = arima_fit.forecast(steps=30)\n",
    "lstm_forecast = lstm_model.predict(X[-30:])\n",
    "lstm_forecast = scaler.inverse_transform(lstm_forecast)\n",
    "\n",
    "# Combine forecasts\n",
    "combined_forecast = (arima_forecast + lstm_forecast.flatten()) / 2\n",
    "print(combined_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7796ede-b391-4cce-8e32-4584dcc4f5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked LSTM-GRU Model\n",
    "# Combine LSTM and GRU layers to capture complex patterns in stock prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79062e62-6429-4a0f-8421-8c549b2f5894",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, GRU, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('stock_prices.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "data = data['Close'].values.reshape(-1, 1)\n",
    "\n",
    "# Scale data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "look_back = 1\n",
    "X, y = [], []\n",
    "for i in range(len(scaled_data)-look_back-1):\n",
    "    X.append(scaled_data[i:(i+look_back), 0])\n",
    "    y.append(scaled_data[i + look_back, 0])\n",
    "X, y = np.array(X), np.array(y)\n",
    "X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "\n",
    "# Build Stacked LSTM-GRU model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(X.shape[1], 1)))\n",
    "model.add(GRU(50, return_sequences=False))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train model\n",
    "model.fit(X, y, epochs=100, batch_size=32, verbose=2)\n",
    "\n",
    "# Predict\n",
    "predicted_stock_price = model.predict(X[-30:])\n",
    "predicted_stock_price = scaler.inverse_transform(predicted_stock_price)\n",
    "print(predicted_stock_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d685449c-33ac-4275-8453-cc03360636a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophet with Sentiment Analysis\n",
    "# Combine Facebook Prophet for time series forecasting with sentiment analysis from news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539fb5b9-76fb-4509-83a1-3193ec261455",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet import Prophet\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('stock_prices.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.rename(columns={'Close': 'y', 'Date': 'ds'}, inplace=True)\n",
    "\n",
    "# Sentiment analysis\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "data['sentiment'] = data['News'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
    "\n",
    "# Add sentiment as a regressor\n",
    "model = Prophet()\n",
    "model.add_regressor('sentiment')\n",
    "model.fit(data)\n",
    "\n",
    "# Make future dataframe for predictions\n",
    "future = model.make_future_dataframe(periods=30)\n",
    "future['sentiment'] = data['sentiment'].iloc[-30:].values\n",
    "\n",
    "# Predict\n",
    "forecast = model.predict(future)\n",
    "print(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c54523f-6a73-4ac4-931d-ed3b5c32abeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost with Technical Indicators\n",
    "# Enhance XGBoost model with additional technical indicators for better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e7a671-d16f-493a-b3ce-9bded88c9ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import talib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('stock_prices.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Technical indicators\n",
    "data['SMA'] = talib.SMA(data['Close'], timeperiod=30)\n",
    "data['RSI'] = talib.RSI(data['Close'], timeperiod=14)\n",
    "data['MACD'], data['MACD_signal'], data['MACD_hist'] = talib.MACD(data['Close'], fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Prepare data\n",
    "X = data[['Open', 'High', 'Low', 'Volume', 'SMA', 'RSI', 'MACD', 'MACD_signal']]\n",
    "y = data['Close']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train XGBoost model\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91e752d-ab3b-4b57-ae83-e9cc774378f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Method: LSTM, GRU, and XGBoost\n",
    "# Combine LSTM, GRU, and XGBoost models in an ensemble to capture diverse patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e15a0a-27fc-46a2-a51e-dfbceda118a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, GRU, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('stock_prices.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "data = data['Close'].values.reshape(-1, 1)\n",
    "\n",
    "# Scale data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "look_back = 1\n",
    "X, y = [], []\n",
    "for i in range(len(scaled_data)-look_back-1):\n",
    "    X.append(scaled_data[i:(i+look_back), 0])\n",
    "    y.append(scaled_data[i + look_back, 0])\n",
    "X, y = np.array(X), np.array(y)\n",
    "X = np.reshape(X,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847885e4-6536-4376-b5e7-77602cfeb024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b018bc6-e872-4b06-80b0-afdba9485f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69de51c9-704c-4014-a5ab-83fd03a505df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3391c1d-0dc6-4b53-afb3-16610ef649fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0dddab-301b-4eb0-b6f2-0a4fde6c5212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4307cf6-6572-441b-8634-248251f089a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
